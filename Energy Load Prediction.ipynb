{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"EEB.jpg\" width=\"800\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accurate quantitative estimation of energy performance of residential buildings using statistical machine learning tools\n",
    "\n",
    "### Source -- Oxford Centre for Industrial and Applied Mathematics, University of Oxford, UK\n",
    "\n",
    "#### Data Set Information:\n",
    "We perform energy analysis using 12 different building shapes simulated in Ecotect. The buildings differ with respect to the glazing area, the glazing area distribution, and the orientation, amongst other parameters. We simulate various settings as functions of the afore-mentioned characteristics to obtain 768 building shapes. The dataset comprises 768 samples and 8 features, aiming to predict two real valued responses. Then the Energy Load of building is classified as LOW , MEDIUM And HIGH based on the predicted real valued responses\n",
    "\n",
    "#### Attribute Information:\n",
    "\n",
    "The dataset contains eight attributes (or features, denoted by X1...X8) and two responses (or outcomes, denoted by y1 and y2). The aim is to use the eight features to predict each of the two responses.\n",
    "\n",
    "Specifically:<br/>\n",
    "X1 Relative Compactness <br/>\n",
    "X2 Surface Area <br/>\n",
    "X3 Wall Area <br/>\n",
    "X4 Roof Area <br/>\n",
    "X5 Overall Height<br/>\n",
    "X6 Orientation<br/>\n",
    "X7 Glazing Area<br/>\n",
    "X8 Glazing Area Distribution<br/>\n",
    "y1 Heating Load<br/>\n",
    "y2 Cooling Load<br/>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  Basic Explorataory analysis was performed on the dataset.\n",
    "*  Since there are two dependent variables I have evaluated two models one with heat_loading and the other with cool_loading.\n",
    "*  Basic models such as linear regression ,Ridge Regression , lasso regression,decision Tree Regressor were evaluate and their R2 score was computed.\n",
    "*  As expected Decision Tree has the highest test and train scores and it is over fitting. \n",
    "*  Ensemble methods such as Random Forest , Bagging , Ada boost were used on both the models and their test and train score were evaluated.\n",
    "*   At last using Neural Network I  have classified the combined heating and cooling load (total load) as low,medium,high with a 90.91% Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"ENB2012_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>Y1</th>\n",
       "      <th>Y2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.98</td>\n",
       "      <td>514.5</td>\n",
       "      <td>294.0</td>\n",
       "      <td>110.25</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.55</td>\n",
       "      <td>21.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.98</td>\n",
       "      <td>514.5</td>\n",
       "      <td>294.0</td>\n",
       "      <td>110.25</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.55</td>\n",
       "      <td>21.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.98</td>\n",
       "      <td>514.5</td>\n",
       "      <td>294.0</td>\n",
       "      <td>110.25</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.55</td>\n",
       "      <td>21.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.98</td>\n",
       "      <td>514.5</td>\n",
       "      <td>294.0</td>\n",
       "      <td>110.25</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.55</td>\n",
       "      <td>21.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.90</td>\n",
       "      <td>563.5</td>\n",
       "      <td>318.5</td>\n",
       "      <td>122.50</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>20.84</td>\n",
       "      <td>28.28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     X1     X2     X3      X4   X5  X6   X7  X8     Y1     Y2\n",
       "0  0.98  514.5  294.0  110.25  7.0   2  0.0   0  15.55  21.33\n",
       "1  0.98  514.5  294.0  110.25  7.0   3  0.0   0  15.55  21.33\n",
       "2  0.98  514.5  294.0  110.25  7.0   4  0.0   0  15.55  21.33\n",
       "3  0.98  514.5  294.0  110.25  7.0   5  0.0   0  15.55  21.33\n",
       "4  0.90  563.5  318.5  122.50  7.0   2  0.0   0  20.84  28.28"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 768 entries, 0 to 767\n",
      "Data columns (total 10 columns):\n",
      "X1    768 non-null float64\n",
      "X2    768 non-null float64\n",
      "X3    768 non-null float64\n",
      "X4    768 non-null float64\n",
      "X5    768 non-null float64\n",
      "X6    768 non-null int64\n",
      "X7    768 non-null float64\n",
      "X8    768 non-null int64\n",
      "Y1    768 non-null float64\n",
      "Y2    768 non-null float64\n",
      "dtypes: float64(8), int64(2)\n",
      "memory usage: 60.1 KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>Y1</th>\n",
       "      <th>Y2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.00000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.00000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.764167</td>\n",
       "      <td>671.708333</td>\n",
       "      <td>318.500000</td>\n",
       "      <td>176.604167</td>\n",
       "      <td>5.25000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>0.234375</td>\n",
       "      <td>2.81250</td>\n",
       "      <td>22.307201</td>\n",
       "      <td>24.587760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.105777</td>\n",
       "      <td>88.086116</td>\n",
       "      <td>43.626481</td>\n",
       "      <td>45.165950</td>\n",
       "      <td>1.75114</td>\n",
       "      <td>1.118763</td>\n",
       "      <td>0.133221</td>\n",
       "      <td>1.55096</td>\n",
       "      <td>10.090196</td>\n",
       "      <td>9.513306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.620000</td>\n",
       "      <td>514.500000</td>\n",
       "      <td>245.000000</td>\n",
       "      <td>110.250000</td>\n",
       "      <td>3.50000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>6.010000</td>\n",
       "      <td>10.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.682500</td>\n",
       "      <td>606.375000</td>\n",
       "      <td>294.000000</td>\n",
       "      <td>140.875000</td>\n",
       "      <td>3.50000</td>\n",
       "      <td>2.750000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.75000</td>\n",
       "      <td>12.992500</td>\n",
       "      <td>15.620000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>673.750000</td>\n",
       "      <td>318.500000</td>\n",
       "      <td>183.750000</td>\n",
       "      <td>5.25000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>18.950000</td>\n",
       "      <td>22.080000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.830000</td>\n",
       "      <td>741.125000</td>\n",
       "      <td>343.000000</td>\n",
       "      <td>220.500000</td>\n",
       "      <td>7.00000</td>\n",
       "      <td>4.250000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>31.667500</td>\n",
       "      <td>33.132500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.980000</td>\n",
       "      <td>808.500000</td>\n",
       "      <td>416.500000</td>\n",
       "      <td>220.500000</td>\n",
       "      <td>7.00000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>43.100000</td>\n",
       "      <td>48.030000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               X1          X2          X3          X4         X5          X6  \\\n",
       "count  768.000000  768.000000  768.000000  768.000000  768.00000  768.000000   \n",
       "mean     0.764167  671.708333  318.500000  176.604167    5.25000    3.500000   \n",
       "std      0.105777   88.086116   43.626481   45.165950    1.75114    1.118763   \n",
       "min      0.620000  514.500000  245.000000  110.250000    3.50000    2.000000   \n",
       "25%      0.682500  606.375000  294.000000  140.875000    3.50000    2.750000   \n",
       "50%      0.750000  673.750000  318.500000  183.750000    5.25000    3.500000   \n",
       "75%      0.830000  741.125000  343.000000  220.500000    7.00000    4.250000   \n",
       "max      0.980000  808.500000  416.500000  220.500000    7.00000    5.000000   \n",
       "\n",
       "               X7         X8          Y1          Y2  \n",
       "count  768.000000  768.00000  768.000000  768.000000  \n",
       "mean     0.234375    2.81250   22.307201   24.587760  \n",
       "std      0.133221    1.55096   10.090196    9.513306  \n",
       "min      0.000000    0.00000    6.010000   10.900000  \n",
       "25%      0.100000    1.75000   12.992500   15.620000  \n",
       "50%      0.250000    3.00000   18.950000   22.080000  \n",
       "75%      0.400000    4.00000   31.667500   33.132500  \n",
       "max      0.400000    5.00000   43.100000   48.030000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data.drop(['Y1','Y2'],axis=1)\n",
    "y=data[['Y1','Y2']]\n",
    "y1=data['Y1']\n",
    "y2=data['Y2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.00000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.764167</td>\n",
       "      <td>671.708333</td>\n",
       "      <td>318.500000</td>\n",
       "      <td>176.604167</td>\n",
       "      <td>5.25000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>0.234375</td>\n",
       "      <td>2.81250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.105777</td>\n",
       "      <td>88.086116</td>\n",
       "      <td>43.626481</td>\n",
       "      <td>45.165950</td>\n",
       "      <td>1.75114</td>\n",
       "      <td>1.118763</td>\n",
       "      <td>0.133221</td>\n",
       "      <td>1.55096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.620000</td>\n",
       "      <td>514.500000</td>\n",
       "      <td>245.000000</td>\n",
       "      <td>110.250000</td>\n",
       "      <td>3.50000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.682500</td>\n",
       "      <td>606.375000</td>\n",
       "      <td>294.000000</td>\n",
       "      <td>140.875000</td>\n",
       "      <td>3.50000</td>\n",
       "      <td>2.750000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.75000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>673.750000</td>\n",
       "      <td>318.500000</td>\n",
       "      <td>183.750000</td>\n",
       "      <td>5.25000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>3.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.830000</td>\n",
       "      <td>741.125000</td>\n",
       "      <td>343.000000</td>\n",
       "      <td>220.500000</td>\n",
       "      <td>7.00000</td>\n",
       "      <td>4.250000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>4.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.980000</td>\n",
       "      <td>808.500000</td>\n",
       "      <td>416.500000</td>\n",
       "      <td>220.500000</td>\n",
       "      <td>7.00000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>5.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               X1          X2          X3          X4         X5          X6  \\\n",
       "count  768.000000  768.000000  768.000000  768.000000  768.00000  768.000000   \n",
       "mean     0.764167  671.708333  318.500000  176.604167    5.25000    3.500000   \n",
       "std      0.105777   88.086116   43.626481   45.165950    1.75114    1.118763   \n",
       "min      0.620000  514.500000  245.000000  110.250000    3.50000    2.000000   \n",
       "25%      0.682500  606.375000  294.000000  140.875000    3.50000    2.750000   \n",
       "50%      0.750000  673.750000  318.500000  183.750000    5.25000    3.500000   \n",
       "75%      0.830000  741.125000  343.000000  220.500000    7.00000    4.250000   \n",
       "max      0.980000  808.500000  416.500000  220.500000    7.00000    5.000000   \n",
       "\n",
       "               X7         X8  \n",
       "count  768.000000  768.00000  \n",
       "mean     0.234375    2.81250  \n",
       "std      0.133221    1.55096  \n",
       "min      0.000000    0.00000  \n",
       "25%      0.100000    1.75000  \n",
       "50%      0.250000    3.00000  \n",
       "75%      0.400000    4.00000  \n",
       "max      0.400000    5.00000  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x000000CFCD56A400>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x000000CFCD79A588>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x000000CFCD7D4588>],\n",
       "       [<matplotlib.axes._subplots.AxesSubplot object at 0x000000CFCD80CAC8>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x000000CFCD845A90>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x000000CFCD845AC8>],\n",
       "       [<matplotlib.axes._subplots.AxesSubplot object at 0x000000CFCD8A8EF0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x000000CFCD8E3EF0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x000000CFCD91DEF0>]], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeoAAAHiCAYAAAAnPo9XAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wFMRndZ22GS5QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xcfcd2fa1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X.hist(bins=8,figsize=(8,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 10)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 118.,  204.,   61.,   29.,   73.,   94.,   98.,   56.,   21.,   14.]),\n",
       " array([ 10.9  ,  14.613,  18.326,  22.039,  25.752,  29.465,  33.178,\n",
       "         36.891,  40.604,  44.317,  48.03 ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAEPBJREFUeJzt3XGsnXV9x/H3Z4iyqRGwF9YA3QVT\njWi0zhtCwjQIzoASwWU40Gl1xGoCmSYuW2XJqCYmbBOZyzaWOhpKgggTETLYtOmczGSgLSCUFSew\nCrVNewUFDIal5bs/7nPlUG57z73nnHsLv/crOTnP8zu/c54vv+R8+uN3n/M8qSokSS9+v7bYBUiS\nFoaBL0mNMPAlqREGviQ1wsCXpEYY+JLUCANfkhph4EtSIwx8SWrESxa7AIAlS5bU+Pj4YpchSS8o\nmzdv/mlVjfXb/6AI/PHxcTZt2rTYZUjSC0qSH8+lv0s6ktQIA1+SGmHgS1IjDHxJaoSBL0mNMPAl\nqREGviQ1wsCXpEYY+JLUiFl/aZvkOOBq4DeBZ4C1VfWlJEcC1wHjwDbg/VX1syQBvgS8G3gK+EhV\n3Tma8hu35lULeKzHF+5Ykkainxn+HuDTVfV64GTgwiQnAquBjVW1HNjY7QOcCSzvHquAK4ZetSRp\nzmYN/KraOT1Dr6onga3AMcDZwPqu23rgnG77bODqmnI7cHiSpUOvXJI0J3Naw08yDrwFuAM4uqp2\nwtQ/CsBRXbdjgEd63ra9a5MkLaK+Az/JK4AbgE9V1RMH6jpDW83weauSbEqyaXJyst8yJEnz1Ffg\nJzmUqbC/pqq+3jXvml6q6Z53d+3bgeN63n4ssGPfz6yqtVU1UVUTY2N9X85ZkjRPswZ+d9bNlcDW\nqvpiz0s3Ayu77ZXATT3tH86Uk4HHp5d+JEmLp58boJwCfAi4N8ndXdvFwKXA9UkuAB4Gzu1eu5Wp\nUzIfYOq0zI8OtWJJ0rzMGvhV9V1mXpcHOH2G/gVcOGBdkqQh85e2ktQIA1+SGmHgS1IjDHxJaoSB\nL0mNMPAlqREGviQ1wsCXpEb080tbdcZX37Jgx9p26XsW7FiS2uAMX5IaYeBLUiMMfElqhIEvSY0w\n8CWpEQa+JDXCwJekRvRzi8N1SXYn2dLTdl2Su7vHtuk7YSUZT/LLntf+cZTFS5L6188Pr64C/g64\nerqhqv5gejvJZcDjPf0frKoVwypQkjQc/dzi8LYk4zO91t3g/P3AacMtS5I0bIOu4b8N2FVVP+pp\nOz7JXUm+k+RtA36+JGlIBr2WzvnAtT37O4FlVfVokrcC30jyhqp6Yt83JlkFrAJYtmzZgGVIkmYz\n7xl+kpcAvwdcN91WVU9X1aPd9mbgQeC1M72/qtZW1URVTYyNjc23DElSnwZZ0nkncH9VbZ9uSDKW\n5JBu+wRgOfDQYCVKkoahn9MyrwX+C3hdku1JLuheOo/nLucAvB24J8kPgK8Bn6iqx4ZZsCRpfvo5\nS+f8/bR/ZIa2G4AbBi9LkjRs/tJWkhph4EtSIwx8SWqEgS9JjTDwJakRBr4kNcLAl6RGGPiS1AgD\nX5IaYeBLUiMMfElqhIEvSY0w8CWpEQa+JDXCwJekRhj4ktQIA1+SGjHrHa+SrAPOAnZX1Ru7tjXA\nx4DJrtvFVXVr99pngAuAvcAfV9U3R1D3i9746ltm7bPtsAUoRNKLRj8z/KuAM2Zov7yqVnSP6bA/\nkal73b6he88/TN/UXJK0uGYN/Kq6Dej3RuRnA1+tqqer6n+BB4CTBqhPkjQkg6zhX5TkniTrkhzR\ntR0DPNLTZ3vXJklaZPMN/CuA1wArgJ3AZV17ZuhbM31AklVJNiXZNDk5OVMXSdIQzSvwq2pXVe2t\nqmeAL/Psss124LierscCO/bzGWuraqKqJsbGxuZThiRpDuYV+EmW9uy+D9jSbd8MnJfkZUmOB5YD\n3xusREnSMPRzWua1wKnAkiTbgUuAU5OsYGq5ZhvwcYCqui/J9cB/A3uAC6tq72hKlyTNxayBX1Xn\nz9B85QH6fx74/CBFSZKGz1/aSlIjDHxJaoSBL0mNMPAlqREGviQ1wsCXpEYY+JLUCANfkhph4EtS\nIwx8SWqEgS9JjTDwJakRBr4kNcLAl6RGGPiS1AgDX5IaMWvgJ1mXZHeSLT1tf53k/iT3JLkxyeFd\n+3iSXya5u3v84yiLlyT1r58Z/lXAGfu0bQDeWFVvAv4H+EzPaw9W1Yru8YnhlClJGtSsgV9VtwGP\n7dP2rara0+3eDhw7gtokSUM0jDX8PwL+tWf/+CR3JflOkrcN4fMlSUMw603MDyTJnwN7gGu6pp3A\nsqp6NMlbgW8keUNVPTHDe1cBqwCWLVs2SBmSpD7Me4afZCVwFvDBqiqAqnq6qh7ttjcDDwKvnen9\nVbW2qiaqamJsbGy+ZUiS+jSvwE9yBvBnwHur6qme9rEkh3TbJwDLgYeGUagkaTCzLukkuRY4FViS\nZDtwCVNn5bwM2JAE4PbujJy3A59LsgfYC3yiqh6b8YMlSQtq1sCvqvNnaL5yP31vAG4YtChJ0vD5\nS1tJaoSBL0mNGOi0TD3ftsM+sNglSNKMnOFLUiMMfElqhIEvSY0w8CWpEQa+JDXCwJekRhj4ktQI\nA1+SGmHgS1IjDHxJaoSBL0mNMPAlqREGviQ1wsCXpEb0FfhJ1iXZnWRLT9uRSTYk+VH3fETXniR/\nm+SBJPck+e1RFS9J6l+/M/yrgDP2aVsNbKyq5cDGbh/gTKZuXr4cWAVcMXiZkqRB9RX4VXUbsO/N\nyM8G1nfb64Fzetqvrim3A4cnWTqMYiVJ8zfIGv7RVbUToHs+qms/Bnikp9/2ru05kqxKsinJpsnJ\nyQHKkCT1YxR/tM0MbfW8hqq1VTVRVRNjY2MjKEOS1GuQe9ruSrK0qnZ2Sza7u/btwHE9/Y4Fdgxw\nHElzseZVC3isxxfuWBrYIDP8m4GV3fZK4Kae9g93Z+ucDDw+vfQjSVo8fc3wk1wLnAosSbIduAS4\nFLg+yQXAw8C5XfdbgXcDDwBPAR8dcs2SpHnoK/Cr6vz9vHT6DH0LuHCQoiRJwzfIGr6kfi3kurq0\nH15aQZIaYeBLUiMMfElqhIEvSY0w8CWpEQa+JDXCwJekRhj4ktQIA1+SGmHgS1IjDHxJaoTX0pGG\nYHz1LQd8fdthC1SIdADO8CWpEQa+JDXCwJekRsx7DT/J64DreppOAP4COBz4GDDZtV9cVbfOu0JJ\n0lDMO/Cr6ofACoAkhwA/AW5k6paGl1fVF4ZSoSRpKIa1pHM68GBV/XhInydJGrJhBf55wLU9+xcl\nuSfJuiRHDOkYkqQBDBz4SV4KvBf4567pCuA1TC337AQu28/7ViXZlGTT5OTkTF0kSUM0jBn+mcCd\nVbULoKp2VdXeqnoG+DJw0kxvqqq1VTVRVRNjY2NDKEOSdCDD+KXt+fQs5yRZWlU7u933AVuGcAxp\nNNa8aigf4y9p9UIwUOAn+Q3gd4GP9zT/VZIVQAHb9nlNkrRIBgr8qnoKePU+bR8aqCJJ0kj4S1tJ\naoSBL0mNMPAlqREGviQ1wsCXpEYY+JLUCANfkhph4EtSIwx8SWrEMK6loxYM6Zoz/R3r8YU7ltQQ\nZ/iS1Ahn+HrRGl99y6x9vMrlYPoZ435su/Q9Q/kcHZgzfElqhIEvSY1wSUfSvG077APD+aA1/fTx\nj/mDcoYvSY0YeIafZBvwJLAX2FNVE0mOBK4Dxpm669X7q+pngx5LkjR/w5rhv6OqVlTVRLe/GthY\nVcuBjd2+JGkRjWpJ52xgfbe9HjhnRMeRJPVpGIFfwLeSbE6yqms7uqp2AnTPRw3hOJKkAQzjLJ1T\nqmpHkqOADUnu7+dN3T8OqwCWLVs2hDIkSQcy8Ay/qnZ0z7uBG4GTgF1JlgJ0z7tneN/aqpqoqomx\nsbFBy5AkzWKgwE/y8iSvnN4G3gVsAW4GVnbdVgI3DXIcSdLgBl3SORq4Mcn0Z32lqv4tyfeB65Nc\nADwMnDvgcSRJAxoo8KvqIeDNM7Q/Cpw+yGdLUi8v1DY4f2krSY0w8CWpEQa+JDXCwJekRhj4ktQI\nA1+SGuENUHTQGdbpd5Keyxm+JDXCwJekRhj4ktQIA1+SGmHgS1IjDHxJaoSBL0mNMPAlqREGviQ1\nwl/a6qCz7bAPLHYJ0ovSvGf4SY5L8u0kW5Pcl+STXfuaJD9Jcnf3ePfwypUkzdcgM/w9wKer6s7u\nRuabk2zoXru8qr4weHmSpGGZd+BX1U5gZ7f9ZJKtwDHDKkySNFxD+aNtknHgLcAdXdNFSe5Jsi7J\nEft5z6okm5JsmpycHEYZkqQDGDjwk7wCuAH4VFU9AVwBvAZYwdT/AVw20/uqam1VTVTVxNjY2KBl\nSJJmMVDgJzmUqbC/pqq+DlBVu6pqb1U9A3wZOGnwMiVJgxrkLJ0AVwJbq+qLPe1Le7q9D9gy//Ik\nScMyyFk6pwAfAu5NcnfXdjFwfpIVQAHbgI8PVKEkMcTfZ6zpp8/jwznWQWaQs3S+C2SGl26dfznz\n4y3xJGl2XlpBkhph4EtSIwx8SWqEgS9JjTDwJakRBr4kNcLr4UvSvta8agGPtXDn/DvDl6RGGPiS\n1AgDX5IaYeBLUiMMfElqhIEvSY0w8CWpEQa+JDXCwJekRows8JOckeSHSR5IsnpUx5Ek9WckgZ/k\nEODvgTOBE5m67eGJoziWJKk/o5rhnwQ8UFUPVdX/AV8Fzh7RsSRJfRhV4B8DPNKzv71rkyQtklFd\nLXOmm5vXczokq4BV3e4vkvxwRLWMwhLgpzO9MNN/eKP2O0YCHJ9+tDFGn513aiwBfmsubxhV4G8H\njuvZPxbY0duhqtYCa0d0/JFKsqmqJha7joOZY3Rgjs/sHKMD68ZnfC7vGdWSzveB5UmOT/JS4Dzg\n5hEdS5LUh5HM8KtqT5KLgG8ChwDrquq+URxLktSfkd3xqqpuBW4d1ecvshfkUtQCc4wOzPGZnWN0\nYHMen1TV7L0kSS94XlpBkhph4M8iyboku5Ns6Wk7MsmGJD/qno9YzBoXU5Ljknw7ydYk9yX5ZNfu\nGHWSHJbke0l+0I3RZ7v245Pc0Y3Rdd0JDs1KckiSu5L8S7fv+PRIsi3JvUnuTrKpa5vT98zAn91V\nwBn7tK0GNlbVcmBjt9+qPcCnq+r1wMnAhd1lNByjZz0NnFZVbwZWAGckORn4S+Dybox+BlywiDUe\nDD4JbO3Zd3ye7x1VtaLndNU5fc8M/FlU1W3AY/s0nw2s77bXA+csaFEHkaraWVV3dttPMvWFPQbH\n6Fdqyi+63UO7RwGnAV/r2pseoyTHAu8B/qnbD45PP+b0PTPw5+foqtoJU4EHHLXI9RwUkowDbwHu\nwDF6jm654m5gN7ABeBD4eVXt6bq0fvmRvwH+FHim2381js++CvhWks3dlQpgjt+zkZ2WqbYkeQVw\nA/CpqnpiaoKmaVW1F1iR5HDgRuD1M3Vb2KoODknOAnZX1eYkp043z9C1yfHpcUpV7UhyFLAhyf1z\n/QBn+POzK8lSgO559yLXs6iSHMpU2F9TVV/vmh2jGVTVz4H/YOrvHYcnmZ50Pe/yIw05BXhvkm1M\nXVn3NKZm/I5Pj6ra0T3vZmrScBJz/J4Z+PNzM7Cy214J3LSItSyqbq31SmBrVX2x5yXHqJNkrJvZ\nk+TXgXcy9beObwO/33Vrdoyq6jNVdWx3XZjzgH+vqg/i+PxKkpcneeX0NvAuYAtz/J75w6tZJLkW\nOJWpK9PtAi4BvgFcDywDHgbOrap9/7DbhCS/A/wncC/Prr9ezNQ6vmMEJHkTU39QO4SpSdb1VfW5\nJCcwNaM9ErgL+MOqenrxKl183ZLOn1TVWY7Ps7qxuLHbfQnwlar6fJJXM4fvmYEvSY1wSUeSGmHg\nS1IjDHxJaoSBL0mNMPAlqREGviQ1wsCXpEYY+JLUiP8Hb7C2+iLeXmcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xcfcda62c88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist((y1))\n",
    "\n",
    "plt.hist((y2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "X1_train,X1_test,y1_train,y1_test=train_test_split(X,y1,test_size=0.2,random_state=10)\n",
    "X2_train,X2_test,y2_train,y2_test=train_test_split(X,y2,test_size=0.2,random_state=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr=LinearRegression()\n",
    "lr.fit(X1_train,y1_train)\n",
    "lr.fit(X2_train,y2_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.85\n",
      "Train score: 0.88\n",
      "Test score: 0.87\n",
      "Test score: 0.91\n"
     ]
    }
   ],
   "source": [
    "print('Train score: {0:0.2f}'.format(lr.score(X1_train,y1_train)))\n",
    "print('Train score: {0:0.2f}'.format(lr.score(X2_train,y2_train)))\n",
    "print('Test score: {0:0.2f}'.format(lr.score(X1_test,y1_test)))\n",
    "print('Test score: {0:0.2f}'.format(lr.score(X2_test,y2_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 19.38841623,  19.86636923,  14.21466747,  29.85930237,\n",
       "        19.87720821,  10.84766993,  14.82662443,  32.19983858,\n",
       "        31.66665361,  17.81705174,  29.78607694,  16.8635108 ,\n",
       "        13.91131478,  32.46583996,  34.52599644,  19.0400956 ,\n",
       "        39.92213944,  13.72954421,  33.70165426,  13.98689738,\n",
       "        29.69965536,  15.96884688,  31.98260243,  31.63476113,\n",
       "        17.24594051,  27.68270968,  15.34368588,  33.56528524,\n",
       "        29.92244596,  17.59469475,  14.79661768,  15.09936246,\n",
       "        15.78926708,  34.84194526,  32.14224944,  29.34049575,\n",
       "        31.71660105,  12.06808941,  18.12216158,  27.18718105,\n",
       "        17.73106377,  27.54634066,  33.88123406,  32.12181584,\n",
       "        34.36155212,  39.78577042,  17.06636071,  31.93939164,\n",
       "        12.47045982,  35.88824807,  13.65396161,  35.62544171,\n",
       "        15.38689667,  17.66984374,  34.11598434,  18.95367402,\n",
       "        15.96928049,  15.00620423,  14.65378127,  30.01171193,\n",
       "        31.67339026,  18.20858316,  17.80621276,  34.88515605,\n",
       "        20.04594904,  17.11630814,  11.94475014,  33.92444485,\n",
       "        13.20838042,  31.78982648,  40.14493003,  17.71305453,\n",
       "        34.43193316,  14.96299344,  31.84623342,  17.96059101,\n",
       "        27.14397026,  17.55105035,  18.0793844 ,  20.49153023,\n",
       "        31.8922886 ,  17.41468134,  34.79199782,  29.38655093,\n",
       "        15.92563609,  35.98460132,  15.69610885,  31.94223603,\n",
       "        27.32355007,  15.7028455 ,  20.17558141,  29.87923517,\n",
       "        17.94258178,  35.79508984,  19.53562423,  15.35495847,\n",
       "        29.42976172,  34.33877493,  36.25060271,  34.38198572,\n",
       "        13.47657257,  35.30629786,  13.95907145,  12.15451099,\n",
       "        34.65472375,  29.72293336,  29.59330099,  17.23510153,\n",
       "        14.7101961 ,  29.56639005,  36.90210173,  14.47420146,\n",
       "        17.64464219,  38.04156277,  32.25144821,  13.37712125,\n",
       "        16.10564951,  15.08178683,  18.39533322,  14.57382708,\n",
       "        31.36875975,  29.5500902 ,  15.6709073 ,  32.14898609,\n",
       "        29.52291995,  17.95385437,  12.37949235,  33.47212701,\n",
       "        32.32182925,  32.82499958,  16.90672159,  15.62769651,\n",
       "        32.3285659 ,  20.00273825,  37.67566651,  34.35481547,\n",
       "        37.76882474,  13.30153865,  14.51741225,  31.96266964,\n",
       "        17.75626532,  29.67972257,  37.68240316,  34.56156553,\n",
       "        40.36772063,  20.09999881,  35.71186329,  18.38859658,\n",
       "        13.0720114 ,  12.01814198,  18.34538579,  27.50312987,\n",
       "        14.00228224,  16.1488603 ])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.predict(X1_test)\n",
    "lr.predict(X2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Linear Regression with Scaling\n",
    "scaler = MinMaxScaler()\n",
    "X1_train_scaled = scaler.fit_transform(X1_train)\n",
    "X1_test_scaled = scaler.transform(X1_test)\n",
    "X2_train_scaled = scaler.fit_transform(X2_train)\n",
    "X2_test_scaled = scaler.transform(X2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear reg1 Train score: 0.9123\n",
      "Linear reg1 Train score: 0.8110\n",
      "Linear reg2 Test score: 0.8660\n",
      "Linear reg2 Test score: 0.9080\n"
     ]
    }
   ],
   "source": [
    "lrs1=LinearRegression()\n",
    "lrs2=LinearRegression()\n",
    "lrs1.fit(X1_train_scaled,y1_train)\n",
    "lrs2.fit(X2_train_scaled,y2_train)\n",
    "print('Linear reg1 Train score: {0:0.4f}'.format(lrs1.score(X1_train_scaled,y1_train)))\n",
    "print('Linear reg1 Train score: {0:0.4f}'.format(lrs1.score(X2_train_scaled,y2_train)))\n",
    "print('Linear reg2 Test score: {0:0.4f}'.format(lrs2.score(X1_test_scaled,y1_test)))\n",
    "print('Linear reg2 Test score: {0:0.4f}'.format(lrs2.score(X2_test_scaled,y2_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "   normalize=False, random_state=None, solver='auto', tol=0.001),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'alpha': [0.001, 0.01, 0.1, 1, 10, 100]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "param_grid = {'alpha':[0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "ridge=Ridge()              \n",
    "\n",
    "grid_search1 = GridSearchCV(ridge, param_grid, cv=5)\n",
    "grid_search2 = GridSearchCV(ridge, param_grid, cv=5)\n",
    "\n",
    "grid_search1.fit(X1_train, y1_train)\n",
    "grid_search2.fit(X2_train, y2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters model1{'alpha': 0.001}\n",
      "Best parameters model2{'alpha': 0.001}\n",
      "Best score model1 0.91\n",
      "Best score model2 0.88\n"
     ]
    }
   ],
   "source": [
    "print('Best parameters model1{}'.format(grid_search1.best_params_))\n",
    "print('Best parameters model2{}'.format(grid_search2.best_params_))\n",
    "print('Best score model1 {:.2f}'.format(grid_search1.best_score_))\n",
    "print('Best score model2 {:.2f}'.format(grid_search2.best_score_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ridge regression linear model1 intercept: 28.941003644318616\n",
      "ridge regression linear model1 coeff:\n",
      "[-22.22266313  -8.01226663   0.88868768 -11.3742237   15.37787437\n",
      "  -0.05571524   8.15788575   1.05825817]\n",
      "Number of non-zero features: 8\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "linridge1 = Ridge(alpha=0.001).fit(X1_train_scaled, y1_train)\n",
    "linridge2 = Ridge(alpha=0.001).fit(X2_train_scaled, y2_train)\n",
    "\n",
    "\n",
    "print('ridge regression linear model1 intercept: {}'\n",
    "     .format(linridge1.intercept_))\n",
    "print('ridge regression linear model1 coeff:\\n{}'\n",
    "     .format(linridge1.coef_))\n",
    "\n",
    "print('Number of non-zero features: {}'\n",
    "     .format(np.sum(linridge1.coef_ != 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge1 R-squared score (training): 0.912\n",
      "Ridge2 R-squared score (training): 0.882\n",
      "Ridge1 R-squared score (test): 0.849\n",
      "Ridge2 R-squared score (test): 0.908\n"
     ]
    }
   ],
   "source": [
    "print('Ridge1 R-squared score (training): {:.3f}'\n",
    "     .format(linridge1.score(X1_train_scaled, y1_train)))\n",
    "print('Ridge2 R-squared score (training): {:.3f}'\n",
    "     .format(linridge2.score(X2_train_scaled, y2_train)))\n",
    "print('Ridge1 R-squared score (test): {:.3f}'\n",
    "     .format(linridge1.score(X1_test_scaled, y2_test)))\n",
    "print('Ridge2 R-squared score (test): {:.3f}'\n",
    "     .format(linridge2.score(X1_test_scaled, y2_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=10000,\n",
       "   normalize=False, positive=False, precompute=False, random_state=None,\n",
       "   selection='cyclic', tol=0.0001, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'alpha': [0.001, 0.01, 0.1, 1, 10, 100]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "param_grid = {'alpha':[0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "lasso=Lasso(max_iter=10000)              \n",
    "\n",
    "grid_searchls1 = GridSearchCV(lasso, param_grid, cv=5)\n",
    "grid_searchls2 = GridSearchCV(lasso, param_grid, cv=5)\n",
    "\n",
    "grid_searchls1.fit(X1_train_scaled, y1_train)\n",
    "grid_searchls2.fit(X2_train_scaled, y2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters model1{'alpha': 0.001}\n",
      "Best parameters model2{'alpha': 0.001}\n",
      "Best score model1 0.91\n",
      "Best score model2 0.88\n"
     ]
    }
   ],
   "source": [
    "print('Best parameters model1{}'.format(grid_searchls1.best_params_))\n",
    "print('Best parameters model2{}'.format(grid_searchls2.best_params_))\n",
    "print('Best score model1 {:.2f}'.format(grid_searchls1.best_score_))\n",
    "print('Best score model2 {:.2f}'.format(grid_searchls2.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso R-squared model1 value Train:0.85\n",
      "Lasso R-squared model2 value Train:0.88\n",
      " Lasso R-squared model1 value Test:0.87\n",
      " Lasso R-squared model2 value Test:0.91\n"
     ]
    }
   ],
   "source": [
    "lasso = Lasso(max_iter = 10000,alpha=0.001)\n",
    "ls1=lasso.fit(X1_train_scaled, y1_train)\n",
    "ls2=lasso.fit(X2_train_scaled, y2_train)\n",
    "\n",
    "print(\"Lasso R-squared model1 value Train:{:.2f}\".format(lasso.score(X1_train_scaled, y1_train)))\n",
    "print(\"Lasso R-squared model2 value Train:{:.2f}\".format(lasso.score(X2_train_scaled, y2_train)))\n",
    "print(\" Lasso R-squared model1 value Test:{:.2f}\".format(lasso.score(X1_test_scaled, y1_test)))\n",
    "print(\" Lasso R-squared model2 value Test:{:.2f}\".format(lasso.score(X1_test_scaled, y2_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters model1{'max_depth': 7, 'max_leaf_nodes': None, 'min_samples_leaf': 1, 'min_samples_split': 20}\n",
      "Best parameters model2{'max_depth': 7, 'max_leaf_nodes': None, 'min_samples_leaf': 1, 'min_samples_split': 20}\n",
      "Best score model1 0.96\n",
      "Best score model2 0.96\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "parameters = {'max_depth':range(3,20), \"min_samples_split\": [2, 10, 20], \"min_samples_leaf\": [1, 5, 10], \n",
    "              \"max_leaf_nodes\": [None, 5, 10, 20]}\n",
    "grid = GridSearchCV(DecisionTreeRegressor(), parameters,)\n",
    "grid_result1 = grid.fit(X1_train_scaled, y1_train)\n",
    "grid_result2 = grid.fit(X2_train_scaled, y2_train)\n",
    "print('Best parameters model1{}'.format(grid_result1.best_params_))\n",
    "print('Best parameters model2{}'.format(grid_result2.best_params_))\n",
    "print('Best score model1 {:.2f}'.format(grid_result1.best_score_))\n",
    "print('Best score model2 {:.2f}'.format(grid_result2.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.9967\n",
      "Train score: 0.9722\n",
      "Test score: 0.9968\n",
      "Test score: 0.9652\n"
     ]
    }
   ],
   "source": [
    "dt_clf1 = DecisionTreeRegressor(max_depth=7, max_leaf_nodes=None, min_samples_leaf=5, min_samples_split= 2)\n",
    "dt_clf2 = DecisionTreeRegressor(max_depth=7, max_leaf_nodes=None, min_samples_leaf=5, min_samples_split= 2)\n",
    "dt_clf1.fit(X1_train_scaled,y1_train)\n",
    "dt_clf2.fit(X2_train_scaled,y2_train)\n",
    "\n",
    "y_dt_clf1 = dt_clf1.predict(X1_test_scaled)\n",
    "print('Train score: {0:0.4f}'.format(dt_clf1.score(X1_train_scaled, y1_train)))\n",
    "print('Train score: {0:0.4f}'.format(dt_clf2.score(X2_train_scaled, y2_train)))\n",
    "print('Test score: {0:0.4f}'.format(dt_clf1.score(X1_test_scaled, y1_test)))\n",
    "print('Test score: {0:0.4f}'.format(dt_clf2.score(X2_test_scaled, y2_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble\n",
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters model1{'max_depth': 25, 'min_samples_leaf': 1, 'n_estimators': 55}\n",
      "Best parameters model2{'max_depth': 25, 'min_samples_leaf': 1, 'n_estimators': 55}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rfc = RandomForestRegressor( max_features='sqrt', oob_score = True) \n",
    "\n",
    "param_grid = { \n",
    "           \"n_estimators\" : range(50,59),\n",
    "           \"max_depth\" : [1, 5, 10, 15, 20, 25, 30],\n",
    "           \"min_samples_leaf\" : [1, 2, 4, 6, 8, 10]}\n",
    "\n",
    "grid = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 10)\n",
    "grid_result3 = grid.fit(X1_train_scaled, y1_train)\n",
    "grid_result4 = grid.fit(X2_train_scaled, y2_train)\n",
    "print('Best parameters model1{}'.format(grid_result3.best_params_))\n",
    "print('Best parameters model2{}'.format(grid_result4.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score model1: 1.00\n",
      "Train score model2: 0.99\n",
      "Test score model1: 1.00\n",
      "Test score model2: 0.97\n"
     ]
    }
   ],
   "source": [
    "rf_clf1 = RandomForestRegressor(max_depth=25, min_samples_leaf=2, n_estimators=18)\n",
    "rf_clf2 = RandomForestRegressor(max_depth=25, min_samples_leaf=2, n_estimators=18)\n",
    "rf_clf1.fit(X1_train_scaled,y1_train)\n",
    "rf_clf2.fit(X2_train_scaled,y2_train)\n",
    "\n",
    "y_rf_clf1 = rf_clf.predict(X1_test_scaled)\n",
    "print('Train score model1: {0:0.2f}'.format(rf_clf1.score(X1_train_scaled, y1_train)))\n",
    "print('Train score model2: {0:0.2f}'.format(rf_clf2.score(X2_train_scaled, y2_train)))\n",
    "print('Test score model1: {0:0.2f}'.format(rf_clf1.score(X1_test_scaled, y1_test)))\n",
    "print('Test score model2: {0:0.2f}'.format(rf_clf2.score(X2_test_scaled, y2_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging on Base models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score linear model1: 0.62\n",
      "Train scorelinear model2: 0.50\n",
      "out of bag score linear model1: 0.58\n",
      "out of bag score linear model2: 0.40\n",
      " \n",
      "\n",
      "Train score ridge model1: 0.32\n",
      "Train score ridge model2: 0.49\n",
      "out of bag score ridge model1: 0.27\n",
      "out of bag score ridge model2: 0.40\n",
      " \n",
      "\n",
      "Train score decision tree1: 0.71\n",
      "Train score decision tree2: 0.44\n",
      "out of bag score decision tree1: 0.64\n",
      "out of bag score decision tree2: 0.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SaiKrishna\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\bagging.py:987: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "C:\\Users\\SaiKrishna\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\bagging.py:987: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "C:\\Users\\SaiKrishna\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\bagging.py:987: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "C:\\Users\\SaiKrishna\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\bagging.py:987: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "C:\\Users\\SaiKrishna\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\bagging.py:987: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "C:\\Users\\SaiKrishna\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\bagging.py:987: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "#bagging on linear and regression models\n",
    "scaler = MinMaxScaler().fit(X)\n",
    "X_scaled = scaler.transform(X)\n",
    "bagginglr1 = BaggingRegressor(lrs1, max_samples=0.8, max_features=0.2, bootstrap = True, oob_score=True)\n",
    "bagginglr2 = BaggingRegressor(lrs2, max_samples=0.8, max_features=0.2, bootstrap = True, oob_score=True)\n",
    "bagginglg1 = BaggingRegressor(linridge1, max_samples=0.8, max_features=0.2, bootstrap = True, oob_score=True)\n",
    "bagginglg2 = BaggingRegressor(linridge2, max_samples=0.8, max_features=0.2, bootstrap = True, oob_score=True)\n",
    "baggingdt1 = BaggingRegressor(dt_clf1, max_samples=0.8, max_features=0.2, bootstrap = True, oob_score=True)\n",
    "baggingdt2 = BaggingRegressor(dt_clf2, max_samples=0.8, max_features=0.2, bootstrap = True, oob_score=True)\n",
    "\n",
    "bagginglr1.fit(X_scaled,y1)\n",
    "bagginglr2.fit(X_scaled,y2)\n",
    "bagginglg1.fit(X_scaled,y1)\n",
    "bagginglg2.fit(X_scaled,y2)\n",
    "baggingdt1.fit(X_scaled,y1)\n",
    "baggingdt2.fit(X_scaled,y2)\n",
    "\n",
    "print('Train score linear model1: {0:0.2f}'.format(bagginglr1.score(X_scaled,y1)))\n",
    "print('Train scorelinear model2: {0:0.2f}'.format(bagginglr2.score(X_scaled,y2)))\n",
    "print('out of bag score linear model1: {0:0.2f}'.format(bagginglr1.oob_score_))\n",
    "print('out of bag score linear model2: {0:0.2f}'.format(bagginglr2.oob_score_))\n",
    "print(\" \\n\")\n",
    "print('Train score ridge model1: {0:0.2f}'.format(bagginglg1.score(X_scaled,y1)))\n",
    "print('Train score ridge model2: {0:0.2f}'.format(bagginglg2.score(X_scaled,y2)))\n",
    "print('out of bag score ridge model1: {0:0.2f}'.format(bagginglg1.oob_score_))\n",
    "print('out of bag score ridge model2: {0:0.2f}'.format(bagginglg2.oob_score_))\n",
    "print(\" \\n\")\n",
    "print('Train score decision tree1: {0:0.2f}'.format(baggingdt1.score(X_scaled,y1)))\n",
    "print('Train score decision tree2: {0:0.2f}'.format(baggingdt2.score(X_scaled,y2)))\n",
    "print('out of bag score decision tree1: {0:0.2f}'.format(baggingdt1.oob_score_))\n",
    "print('out of bag score decision tree2: {0:0.2f}'.format(baggingdt2.oob_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score model1: 0.9105\n",
      "Train score model2: 0.8795\n",
      "Test score model1: 0.9245\n",
      "Test score model2: 0.8991\n",
      " \n",
      "\n",
      "Train score model1: 0.9106\n",
      "Train score model2: 0.8791\n",
      "Test score model1: 0.9265\n",
      "Test score model2: 0.9019\n",
      " \n",
      "\n",
      "Train score model1: 0.9984\n",
      "Train score model2: 0.9807\n",
      "Test score model1: 0.9970\n",
      "Test score model2: 0.9701\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "#bagging on linear and regression models\n",
    "scaler = MinMaxScaler().fit(X)\n",
    "X_scaled = scaler.transform(X)\n",
    "#linear\n",
    "adaboost_lr1 = AdaBoostRegressor(base_estimator = lrs1, learning_rate = 0.5)\n",
    "adaboost_lr2 = AdaBoostRegressor(base_estimator = lrs2, learning_rate = 0.5)\n",
    "#ridge\n",
    "adaboost_rg1 = AdaBoostRegressor(base_estimator = linridge1, learning_rate = 0.5)\n",
    "adaboost_rg2 = AdaBoostRegressor(base_estimator = linridge2, learning_rate = 0.5)\n",
    "#decision tree\n",
    "\n",
    "adaboost_dt1 = AdaBoostRegressor(base_estimator = dt_clf1, learning_rate = 0.5)\n",
    "adaboost_dt2 = AdaBoostRegressor(base_estimator = dt_clf2, learning_rate = 0.5)\n",
    "\n",
    "adaboost_lr1.fit(X1_train_scaled, y1_train)\n",
    "adaboost_lr2.fit(X2_train_scaled, y2_train)\n",
    "adaboost_rg1.fit(X1_train_scaled, y1_train)\n",
    "adaboost_rg2.fit(X2_train_scaled, y2_train)\n",
    "adaboost_dt1.fit(X1_train_scaled, y1_train)\n",
    "adaboost_dt2.fit(X2_train_scaled, y2_train)\n",
    "\n",
    "print('Train score model1: {0:0.4f}'.format(adaboost_lr1.score(X1_train_scaled, y1_train)))\n",
    "print('Train score model2: {0:0.4f}'.format(adaboost_lr2.score(X2_train_scaled, y2_train)))\n",
    "print('Test score model1: {0:0.4f}'.format(adaboost_lr1.score(X1_test_scaled, y1_test)))\n",
    "print('Test score model2: {0:0.4f}'.format(adaboost_lr2.score(X2_test_scaled, y2_test)))\n",
    "print(\" \\n\")\n",
    "print('Train score model1: {0:0.4f}'.format(adaboost_rg1.score(X1_train_scaled, y1_train)))\n",
    "print('Train score model2: {0:0.4f}'.format(adaboost_rg2.score(X2_train_scaled, y2_train)))\n",
    "print('Test score model1: {0:0.4f}'.format(adaboost_rg1.score(X1_test_scaled, y1_test)))\n",
    "print('Test score model2: {0:0.4f}'.format(adaboost_rg2.score(X2_test_scaled, y2_test)))\n",
    "print(\" \\n\")\n",
    "print('Train score model1: {0:0.4f}'.format(adaboost_dt1.score(X1_train_scaled, y1_train)))\n",
    "print('Train score model2: {0:0.4f}'.format(adaboost_dt2.score(X2_train_scaled, y2_train)))\n",
    "print('Test score model1: {0:0.4f}'.format(adaboost_dt1.score(X1_test_scaled, y1_test)))\n",
    "print('Test score model2: {0:0.4f}'.format(adaboost_dt2.score(X2_test_scaled, y2_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nerual Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y=y1+y2 #adding hot and cold loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0xcfceed2c88>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAD8CAYAAABthzNFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQpLUZEFIkpr+DyBiTAbLJ0r6AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xcfcddc06a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y.plot(kind='hist',bins=10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40.97"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.median()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46.89496093749997"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.949999999999999"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89.950000000000003"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d=pd.DataFrame(y,columns=['y'])\n",
    "d.loc[d['y']<=40,'y_category']='low'\n",
    "d.loc[(d['y']>40) & (d['y'] < 65),'y_category'] = 'medium'\n",
    "d.loc[d['y'] >= 65,'y_category'] ='high'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,d,test_size=0.2,random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_final_train=pd.get_dummies(data=y_train['y_category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sc=MinMaxScaler()\n",
    "#X_train=sc.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, activation='relu',kernel_initializer='normal'))\n",
    "model.add(Dense(8, activation='relu', kernel_initializer='normal'))\n",
    "model.add(Dense(3, activation='sigmoid', kernel_initializer='normal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "614/614 [==============================] - 1s 854us/step - loss: 0.6126 - acc: 0.6591\n",
      "Epoch 2/200\n",
      "614/614 [==============================] - 0s 83us/step - loss: 0.5874 - acc: 0.6824\n",
      "Epoch 3/200\n",
      "614/614 [==============================] - 0s 77us/step - loss: 0.5628 - acc: 0.7524\n",
      "Epoch 4/200\n",
      "614/614 [==============================] - 0s 77us/step - loss: 0.5223 - acc: 0.7801\n",
      "Epoch 5/200\n",
      "614/614 [==============================] - 0s 82us/step - loss: 0.4609 - acc: 0.8355\n",
      "Epoch 6/200\n",
      "614/614 [==============================] - 0s 81us/step - loss: 0.4020 - acc: 0.8143\n",
      "Epoch 7/200\n",
      "614/614 [==============================] - 0s 75us/step - loss: 0.3622 - acc: 0.8176\n",
      "Epoch 8/200\n",
      "614/614 [==============================] - 0s 78us/step - loss: 0.3375 - acc: 0.8154\n",
      "Epoch 9/200\n",
      "614/614 [==============================] - 0s 77us/step - loss: 0.3231 - acc: 0.8105\n",
      "Epoch 10/200\n",
      "614/614 [==============================] - 0s 80us/step - loss: 0.3148 - acc: 0.8143\n",
      "Epoch 11/200\n",
      "614/614 [==============================] - 0s 77us/step - loss: 0.3132 - acc: 0.8165\n",
      "Epoch 12/200\n",
      "614/614 [==============================] - 0s 77us/step - loss: 0.3088 - acc: 0.8257\n",
      "Epoch 13/200\n",
      "614/614 [==============================] - 0s 77us/step - loss: 0.3038 - acc: 0.8192\n",
      "Epoch 14/200\n",
      "614/614 [==============================] - 0s 73us/step - loss: 0.3077 - acc: 0.8257\n",
      "Epoch 15/200\n",
      "614/614 [==============================] - 0s 77us/step - loss: 0.3049 - acc: 0.8192\n",
      "Epoch 16/200\n",
      "614/614 [==============================] - 0s 77us/step - loss: 0.3010 - acc: 0.8208\n",
      "Epoch 17/200\n",
      "614/614 [==============================] - 0s 77us/step - loss: 0.3008 - acc: 0.8160\n",
      "Epoch 18/200\n",
      "614/614 [==============================] - 0s 73us/step - loss: 0.2968 - acc: 0.8230\n",
      "Epoch 19/200\n",
      "614/614 [==============================] - 0s 73us/step - loss: 0.3006 - acc: 0.8160\n",
      "Epoch 20/200\n",
      "614/614 [==============================] - 0s 73us/step - loss: 0.2972 - acc: 0.8111\n",
      "Epoch 21/200\n",
      "614/614 [==============================] - 0s 75us/step - loss: 0.2973 - acc: 0.8192\n",
      "Epoch 22/200\n",
      "614/614 [==============================] - 0s 77us/step - loss: 0.2963 - acc: 0.8230\n",
      "Epoch 23/200\n",
      "614/614 [==============================] - 0s 77us/step - loss: 0.2987 - acc: 0.8181\n",
      "Epoch 24/200\n",
      "614/614 [==============================] - 0s 77us/step - loss: 0.2999 - acc: 0.8170\n",
      "Epoch 25/200\n",
      "614/614 [==============================] - 0s 78us/step - loss: 0.2986 - acc: 0.8274\n",
      "Epoch 26/200\n",
      "614/614 [==============================] - 0s 77us/step - loss: 0.2973 - acc: 0.8165\n",
      "Epoch 27/200\n",
      "614/614 [==============================] - 0s 78us/step - loss: 0.2966 - acc: 0.8219\n",
      "Epoch 28/200\n",
      "614/614 [==============================] - 0s 85us/step - loss: 0.2945 - acc: 0.8236\n",
      "Epoch 29/200\n",
      "614/614 [==============================] - 0s 86us/step - loss: 0.2975 - acc: 0.8371\n",
      "Epoch 30/200\n",
      "614/614 [==============================] - 0s 106us/step - loss: 0.2972 - acc: 0.8154\n",
      "Epoch 31/200\n",
      "614/614 [==============================] - 0s 95us/step - loss: 0.2999 - acc: 0.8360\n",
      "Epoch 32/200\n",
      "614/614 [==============================] - 0s 98us/step - loss: 0.2939 - acc: 0.8268\n",
      "Epoch 33/200\n",
      "614/614 [==============================] - 0s 96us/step - loss: 0.2951 - acc: 0.8181\n",
      "Epoch 34/200\n",
      "614/614 [==============================] - 0s 98us/step - loss: 0.2972 - acc: 0.8263\n",
      "Epoch 35/200\n",
      "614/614 [==============================] - 0s 95us/step - loss: 0.2957 - acc: 0.8203\n",
      "Epoch 36/200\n",
      "614/614 [==============================] - 0s 101us/step - loss: 0.2949 - acc: 0.8301\n",
      "Epoch 37/200\n",
      "614/614 [==============================] - 0s 98us/step - loss: 0.2945 - acc: 0.8165\n",
      "Epoch 38/200\n",
      "614/614 [==============================] - 0s 103us/step - loss: 0.2955 - acc: 0.8344\n",
      "Epoch 39/200\n",
      "614/614 [==============================] - 0s 98us/step - loss: 0.2928 - acc: 0.8290\n",
      "Epoch 40/200\n",
      "614/614 [==============================] - 0s 104us/step - loss: 0.2924 - acc: 0.8377\n",
      "Epoch 41/200\n",
      "614/614 [==============================] - 0s 93us/step - loss: 0.2944 - acc: 0.8203\n",
      "Epoch 42/200\n",
      "614/614 [==============================] - 0s 93us/step - loss: 0.2914 - acc: 0.8382\n",
      "Epoch 43/200\n",
      "614/614 [==============================] - 0s 98us/step - loss: 0.2913 - acc: 0.8219\n",
      "Epoch 44/200\n",
      "614/614 [==============================] - 0s 98us/step - loss: 0.2949 - acc: 0.8360\n",
      "Epoch 45/200\n",
      "614/614 [==============================] - 0s 98us/step - loss: 0.2919 - acc: 0.8279\n",
      "Epoch 46/200\n",
      "614/614 [==============================] - 0s 101us/step - loss: 0.2905 - acc: 0.8301\n",
      "Epoch 47/200\n",
      "614/614 [==============================] - 0s 98us/step - loss: 0.2933 - acc: 0.8246\n",
      "Epoch 48/200\n",
      "614/614 [==============================] - 0s 95us/step - loss: 0.2902 - acc: 0.8241\n",
      "Epoch 49/200\n",
      "614/614 [==============================] - 0s 98us/step - loss: 0.2919 - acc: 0.8257\n",
      "Epoch 50/200\n",
      "614/614 [==============================] - 0s 96us/step - loss: 0.2911 - acc: 0.8350\n",
      "Epoch 51/200\n",
      "614/614 [==============================] - 0s 83us/step - loss: 0.2906 - acc: 0.8355\n",
      "Epoch 52/200\n",
      "614/614 [==============================] - 0s 96us/step - loss: 0.2975 - acc: 0.8208\n",
      "Epoch 53/200\n",
      "614/614 [==============================] - 0s 95us/step - loss: 0.2941 - acc: 0.8312\n",
      "Epoch 54/200\n",
      "614/614 [==============================] - 0s 96us/step - loss: 0.2903 - acc: 0.8317\n",
      "Epoch 55/200\n",
      "614/614 [==============================] - 0s 86us/step - loss: 0.2867 - acc: 0.8295\n",
      "Epoch 56/200\n",
      "614/614 [==============================] - 0s 99us/step - loss: 0.2899 - acc: 0.8241\n",
      "Epoch 57/200\n",
      "614/614 [==============================] - 0s 95us/step - loss: 0.2834 - acc: 0.8268\n",
      "Epoch 58/200\n",
      "614/614 [==============================] - 0s 103us/step - loss: 0.2836 - acc: 0.8322\n",
      "Epoch 59/200\n",
      "614/614 [==============================] - 0s 88us/step - loss: 0.2833 - acc: 0.8312\n",
      "Epoch 60/200\n",
      "614/614 [==============================] - 0s 101us/step - loss: 0.2815 - acc: 0.8306\n",
      "Epoch 61/200\n",
      "614/614 [==============================] - 0s 95us/step - loss: 0.2810 - acc: 0.8322\n",
      "Epoch 62/200\n",
      "614/614 [==============================] - 0s 103us/step - loss: 0.2780 - acc: 0.8523\n",
      "Epoch 63/200\n",
      "614/614 [==============================] - 0s 98us/step - loss: 0.2804 - acc: 0.8496\n",
      "Epoch 64/200\n",
      "614/614 [==============================] - 0s 96us/step - loss: 0.2783 - acc: 0.8512\n",
      "Epoch 65/200\n",
      "614/614 [==============================] - 0s 95us/step - loss: 0.2743 - acc: 0.8507\n",
      "Epoch 66/200\n",
      "614/614 [==============================] - 0s 96us/step - loss: 0.2731 - acc: 0.8529\n",
      "Epoch 67/200\n",
      "614/614 [==============================] - 0s 93us/step - loss: 0.2744 - acc: 0.8447\n",
      "Epoch 68/200\n",
      "614/614 [==============================] - 0s 93us/step - loss: 0.2764 - acc: 0.8529\n",
      "Epoch 69/200\n",
      "614/614 [==============================] - 0s 85us/step - loss: 0.2717 - acc: 0.8518\n",
      "Epoch 70/200\n",
      "614/614 [==============================] - 0s 88us/step - loss: 0.2716 - acc: 0.8469\n",
      "Epoch 71/200\n",
      "614/614 [==============================] - 0s 96us/step - loss: 0.2689 - acc: 0.8507\n",
      "Epoch 72/200\n",
      "614/614 [==============================] - 0s 95us/step - loss: 0.2693 - acc: 0.8545\n",
      "Epoch 73/200\n",
      "614/614 [==============================] - 0s 85us/step - loss: 0.2658 - acc: 0.8730\n",
      "Epoch 74/200\n",
      "614/614 [==============================] - 0s 95us/step - loss: 0.2692 - acc: 0.8616\n",
      "Epoch 75/200\n",
      "614/614 [==============================] - 0s 98us/step - loss: 0.2653 - acc: 0.8735\n",
      "Epoch 76/200\n",
      "614/614 [==============================] - 0s 104us/step - loss: 0.2636 - acc: 0.8692\n",
      "Epoch 77/200\n",
      "614/614 [==============================] - 0s 83us/step - loss: 0.2634 - acc: 0.8855\n",
      "Epoch 78/200\n",
      "614/614 [==============================] - 0s 78us/step - loss: 0.2648 - acc: 0.8811\n",
      "Epoch 79/200\n",
      "614/614 [==============================] - 0s 78us/step - loss: 0.2676 - acc: 0.8746\n",
      "Epoch 80/200\n",
      "614/614 [==============================] - 0s 93us/step - loss: 0.2634 - acc: 0.8871\n",
      "Epoch 81/200\n",
      "614/614 [==============================] - 0s 78us/step - loss: 0.2608 - acc: 0.8914\n",
      "Epoch 82/200\n",
      "614/614 [==============================] - 0s 82us/step - loss: 0.2626 - acc: 0.8855\n",
      "Epoch 83/200\n",
      "614/614 [==============================] - 0s 82us/step - loss: 0.2616 - acc: 0.8827\n",
      "Epoch 84/200\n",
      "614/614 [==============================] - 0s 75us/step - loss: 0.2626 - acc: 0.8844\n",
      "Epoch 85/200\n",
      "614/614 [==============================] - 0s 75us/step - loss: 0.2588 - acc: 0.8865\n",
      "Epoch 86/200\n",
      "614/614 [==============================] - 0s 81us/step - loss: 0.2621 - acc: 0.8882\n",
      "Epoch 87/200\n",
      "614/614 [==============================] - 0s 84us/step - loss: 0.2578 - acc: 0.8958\n",
      "Epoch 88/200\n",
      "614/614 [==============================] - 0s 77us/step - loss: 0.2620 - acc: 0.8855\n",
      "Epoch 89/200\n",
      "614/614 [==============================] - 0s 77us/step - loss: 0.2564 - acc: 0.9028\n",
      "Epoch 90/200\n",
      "614/614 [==============================] - 0s 83us/step - loss: 0.2540 - acc: 0.9023\n",
      "Epoch 91/200\n",
      "614/614 [==============================] - 0s 83us/step - loss: 0.2573 - acc: 0.8838\n",
      "Epoch 92/200\n",
      "614/614 [==============================] - 0s 77us/step - loss: 0.2552 - acc: 0.9012\n",
      "Epoch 93/200\n",
      "614/614 [==============================] - 0s 78us/step - loss: 0.2592 - acc: 0.8903\n",
      "Epoch 94/200\n",
      "614/614 [==============================] - 0s 78us/step - loss: 0.2567 - acc: 0.8893\n",
      "Epoch 95/200\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.3001 - acc: 0.883 - 0s 80us/step - loss: 0.2586 - acc: 0.8985\n",
      "Epoch 96/200\n",
      "614/614 [==============================] - 0s 96us/step - loss: 0.2555 - acc: 0.8860\n",
      "Epoch 97/200\n",
      "614/614 [==============================] - 0s 101us/step - loss: 0.2516 - acc: 0.8914\n",
      "Epoch 98/200\n",
      "614/614 [==============================] - 0s 96us/step - loss: 0.2516 - acc: 0.9012\n",
      "Epoch 99/200\n",
      "614/614 [==============================] - 0s 83us/step - loss: 0.2498 - acc: 0.9001\n",
      "Epoch 100/200\n",
      "614/614 [==============================] - 0s 91us/step - loss: 0.2539 - acc: 0.8876\n",
      "Epoch 101/200\n",
      "614/614 [==============================] - 0s 93us/step - loss: 0.2505 - acc: 0.9007\n",
      "Epoch 102/200\n",
      "614/614 [==============================] - 0s 85us/step - loss: 0.2503 - acc: 0.8920\n",
      "Epoch 103/200\n",
      "614/614 [==============================] - 0s 83us/step - loss: 0.2506 - acc: 0.8952\n",
      "Epoch 104/200\n",
      "614/614 [==============================] - 0s 95us/step - loss: 0.2479 - acc: 0.8974\n",
      "Epoch 105/200\n",
      "614/614 [==============================] - 0s 81us/step - loss: 0.2507 - acc: 0.8920\n",
      "Epoch 106/200\n",
      "614/614 [==============================] - 0s 80us/step - loss: 0.2499 - acc: 0.9007\n",
      "Epoch 107/200\n",
      "614/614 [==============================] - 0s 81us/step - loss: 0.2474 - acc: 0.8931\n",
      "Epoch 108/200\n",
      "614/614 [==============================] - 0s 81us/step - loss: 0.2494 - acc: 0.8963\n",
      "Epoch 109/200\n",
      "614/614 [==============================] - 0s 90us/step - loss: 0.2641 - acc: 0.8773\n",
      "Epoch 110/200\n",
      "614/614 [==============================] - 0s 104us/step - loss: 0.2514 - acc: 0.8931\n",
      "Epoch 111/200\n",
      "614/614 [==============================] - 0s 83us/step - loss: 0.2494 - acc: 0.9007\n",
      "Epoch 112/200\n",
      "614/614 [==============================] - 0s 83us/step - loss: 0.2479 - acc: 0.8925\n",
      "Epoch 113/200\n",
      "614/614 [==============================] - 0s 91us/step - loss: 0.2456 - acc: 0.8958\n",
      "Epoch 114/200\n",
      "614/614 [==============================] - 0s 78us/step - loss: 0.2442 - acc: 0.9066\n",
      "Epoch 115/200\n",
      "614/614 [==============================] - 0s 85us/step - loss: 0.2476 - acc: 0.8920\n",
      "Epoch 116/200\n",
      "614/614 [==============================] - 0s 78us/step - loss: 0.2580 - acc: 0.8768\n",
      "Epoch 117/200\n",
      "614/614 [==============================] - 0s 85us/step - loss: 0.2537 - acc: 0.8795\n",
      "Epoch 118/200\n",
      "614/614 [==============================] - 0s 80us/step - loss: 0.2525 - acc: 0.8811\n",
      "Epoch 119/200\n",
      "614/614 [==============================] - 0s 91us/step - loss: 0.2467 - acc: 0.8931\n",
      "Epoch 120/200\n",
      "614/614 [==============================] - 0s 86us/step - loss: 0.2491 - acc: 0.8941\n",
      "Epoch 121/200\n",
      "614/614 [==============================] - 0s 86us/step - loss: 0.2450 - acc: 0.8996\n",
      "Epoch 122/200\n",
      "614/614 [==============================] - 0s 85us/step - loss: 0.2491 - acc: 0.8914\n",
      "Epoch 123/200\n",
      "614/614 [==============================] - 0s 93us/step - loss: 0.2437 - acc: 0.9012\n",
      "Epoch 124/200\n",
      "614/614 [==============================] - 0s 104us/step - loss: 0.2473 - acc: 0.8936\n",
      "Epoch 125/200\n",
      "614/614 [==============================] - 0s 96us/step - loss: 0.2431 - acc: 0.9061\n",
      "Epoch 126/200\n",
      "614/614 [==============================] - 0s 81us/step - loss: 0.2483 - acc: 0.8827\n",
      "Epoch 127/200\n",
      "614/614 [==============================] - 0s 81us/step - loss: 0.2426 - acc: 0.9050\n",
      "Epoch 128/200\n",
      "614/614 [==============================] - 0s 95us/step - loss: 0.2459 - acc: 0.8947\n",
      "Epoch 129/200\n",
      "614/614 [==============================] - 0s 81us/step - loss: 0.2445 - acc: 0.8925\n",
      "Epoch 130/200\n",
      "614/614 [==============================] - 0s 91us/step - loss: 0.2421 - acc: 0.8903\n",
      "Epoch 131/200\n",
      "614/614 [==============================] - 0s 81us/step - loss: 0.2425 - acc: 0.8958\n",
      "Epoch 132/200\n",
      "614/614 [==============================] - 0s 93us/step - loss: 0.2416 - acc: 0.9039\n",
      "Epoch 133/200\n",
      "614/614 [==============================] - 0s 81us/step - loss: 0.2454 - acc: 0.8876\n",
      "Epoch 134/200\n",
      "614/614 [==============================] - 0s 88us/step - loss: 0.2413 - acc: 0.8909\n",
      "Epoch 135/200\n",
      "614/614 [==============================] - 0s 90us/step - loss: 0.2426 - acc: 0.9083\n",
      "Epoch 136/200\n",
      "614/614 [==============================] - 0s 80us/step - loss: 0.2439 - acc: 0.8898\n",
      "Epoch 137/200\n",
      "614/614 [==============================] - 0s 86us/step - loss: 0.2409 - acc: 0.8947\n",
      "Epoch 138/200\n",
      "614/614 [==============================] - 0s 81us/step - loss: 0.2408 - acc: 0.8996\n",
      "Epoch 139/200\n",
      "614/614 [==============================] - 0s 80us/step - loss: 0.2456 - acc: 0.8985\n",
      "Epoch 140/200\n",
      "614/614 [==============================] - 0s 83us/step - loss: 0.2447 - acc: 0.8914\n",
      "Epoch 141/200\n",
      "614/614 [==============================] - 0s 83us/step - loss: 0.2408 - acc: 0.8985\n",
      "Epoch 142/200\n",
      "614/614 [==============================] - 0s 81us/step - loss: 0.2402 - acc: 0.8903\n",
      "Epoch 143/200\n",
      "614/614 [==============================] - 0s 80us/step - loss: 0.2477 - acc: 0.8979\n",
      "Epoch 144/200\n",
      "614/614 [==============================] - 0s 83us/step - loss: 0.2424 - acc: 0.9012\n",
      "Epoch 145/200\n",
      "614/614 [==============================] - 0s 78us/step - loss: 0.2451 - acc: 0.9023\n",
      "Epoch 146/200\n",
      "614/614 [==============================] - 0s 80us/step - loss: 0.2412 - acc: 0.9023\n",
      "Epoch 147/200\n",
      "614/614 [==============================] - 0s 80us/step - loss: 0.2406 - acc: 0.8996\n",
      "Epoch 148/200\n",
      "614/614 [==============================] - 0s 83us/step - loss: 0.2410 - acc: 0.8990\n",
      "Epoch 149/200\n",
      "614/614 [==============================] - 0s 80us/step - loss: 0.2421 - acc: 0.8979\n",
      "Epoch 150/200\n",
      "614/614 [==============================] - 0s 80us/step - loss: 0.2475 - acc: 0.8849\n",
      "Epoch 151/200\n",
      "614/614 [==============================] - 0s 78us/step - loss: 0.2384 - acc: 0.9077\n",
      "Epoch 152/200\n",
      "614/614 [==============================] - 0s 80us/step - loss: 0.2403 - acc: 0.8958\n",
      "Epoch 153/200\n",
      "614/614 [==============================] - 0s 80us/step - loss: 0.2458 - acc: 0.9007\n",
      "Epoch 154/200\n",
      "614/614 [==============================] - 0s 85us/step - loss: 0.2422 - acc: 0.8979\n",
      "Epoch 155/200\n",
      "614/614 [==============================] - 0s 83us/step - loss: 0.2391 - acc: 0.8936\n",
      "Epoch 156/200\n",
      "614/614 [==============================] - 0s 78us/step - loss: 0.2411 - acc: 0.9028\n",
      "Epoch 157/200\n",
      "614/614 [==============================] - 0s 81us/step - loss: 0.2390 - acc: 0.9023\n",
      "Epoch 158/200\n",
      "614/614 [==============================] - 0s 86us/step - loss: 0.2393 - acc: 0.9001\n",
      "Epoch 159/200\n",
      "614/614 [==============================] - 0s 80us/step - loss: 0.2421 - acc: 0.8979\n",
      "Epoch 160/200\n",
      "614/614 [==============================] - 0s 78us/step - loss: 0.2412 - acc: 0.9039\n",
      "Epoch 161/200\n",
      "614/614 [==============================] - 0s 81us/step - loss: 0.2382 - acc: 0.9045\n",
      "Epoch 162/200\n",
      "614/614 [==============================] - 0s 80us/step - loss: 0.2378 - acc: 0.9077\n",
      "Epoch 163/200\n",
      "614/614 [==============================] - 0s 78us/step - loss: 0.2377 - acc: 0.9050\n",
      "Epoch 164/200\n",
      "614/614 [==============================] - 0s 80us/step - loss: 0.2395 - acc: 0.9028\n",
      "Epoch 165/200\n",
      "614/614 [==============================] - 0s 83us/step - loss: 0.2372 - acc: 0.9001\n",
      "Epoch 166/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "614/614 [==============================] - 0s 78us/step - loss: 0.2419 - acc: 0.8936\n",
      "Epoch 167/200\n",
      "614/614 [==============================] - 0s 77us/step - loss: 0.2386 - acc: 0.9077\n",
      "Epoch 168/200\n",
      "614/614 [==============================] - 0s 72us/step - loss: 0.2393 - acc: 0.9012\n",
      "Epoch 169/200\n",
      "614/614 [==============================] - 0s 91us/step - loss: 0.2363 - acc: 0.9061\n",
      "Epoch 170/200\n",
      "614/614 [==============================] - 0s 86us/step - loss: 0.2376 - acc: 0.9017\n",
      "Epoch 171/200\n",
      "614/614 [==============================] - 0s 112us/step - loss: 0.2372 - acc: 0.9055\n",
      "Epoch 172/200\n",
      "614/614 [==============================] - 0s 104us/step - loss: 0.2386 - acc: 0.8958\n",
      "Epoch 173/200\n",
      "614/614 [==============================] - 0s 104us/step - loss: 0.2376 - acc: 0.9093\n",
      "Epoch 174/200\n",
      "614/614 [==============================] - 0s 85us/step - loss: 0.2385 - acc: 0.8947\n",
      "Epoch 175/200\n",
      "614/614 [==============================] - 0s 91us/step - loss: 0.2437 - acc: 0.8909\n",
      "Epoch 176/200\n",
      "614/614 [==============================] - 0s 80us/step - loss: 0.2363 - acc: 0.9039\n",
      "Epoch 177/200\n",
      "614/614 [==============================] - 0s 85us/step - loss: 0.2361 - acc: 0.9028\n",
      "Epoch 178/200\n",
      "614/614 [==============================] - 0s 81us/step - loss: 0.2360 - acc: 0.9017\n",
      "Epoch 179/200\n",
      "614/614 [==============================] - 0s 77us/step - loss: 0.2350 - acc: 0.9066\n",
      "Epoch 180/200\n",
      "614/614 [==============================] - 0s 78us/step - loss: 0.2347 - acc: 0.9083\n",
      "Epoch 181/200\n",
      "614/614 [==============================] - 0s 98us/step - loss: 0.2373 - acc: 0.8963\n",
      "Epoch 182/200\n",
      "614/614 [==============================] - 0s 95us/step - loss: 0.2515 - acc: 0.8931\n",
      "Epoch 183/200\n",
      "614/614 [==============================] - 0s 90us/step - loss: 0.2383 - acc: 0.9007\n",
      "Epoch 184/200\n",
      "614/614 [==============================] - 0s 91us/step - loss: 0.2391 - acc: 0.8920\n",
      "Epoch 185/200\n",
      "614/614 [==============================] - 0s 95us/step - loss: 0.2386 - acc: 0.8958\n",
      "Epoch 186/200\n",
      "614/614 [==============================] - 0s 93us/step - loss: 0.2440 - acc: 0.9023\n",
      "Epoch 187/200\n",
      "614/614 [==============================] - 0s 81us/step - loss: 0.2338 - acc: 0.9066\n",
      "Epoch 188/200\n",
      "614/614 [==============================] - 0s 85us/step - loss: 0.2386 - acc: 0.8990\n",
      "Epoch 189/200\n",
      "614/614 [==============================] - 0s 78us/step - loss: 0.2352 - acc: 0.9066\n",
      "Epoch 190/200\n",
      "614/614 [==============================] - 0s 81us/step - loss: 0.2392 - acc: 0.8979\n",
      "Epoch 191/200\n",
      "614/614 [==============================] - 0s 81us/step - loss: 0.2367 - acc: 0.8952\n",
      "Epoch 192/200\n",
      "614/614 [==============================] - 0s 88us/step - loss: 0.2408 - acc: 0.8985\n",
      "Epoch 193/200\n",
      "614/614 [==============================] - 0s 85us/step - loss: 0.2402 - acc: 0.9050\n",
      "Epoch 194/200\n",
      "614/614 [==============================] - 0s 85us/step - loss: 0.2363 - acc: 0.8969\n",
      "Epoch 195/200\n",
      "614/614 [==============================] - 0s 85us/step - loss: 0.2344 - acc: 0.9034\n",
      "Epoch 196/200\n",
      "614/614 [==============================] - 0s 86us/step - loss: 0.2360 - acc: 0.8985\n",
      "Epoch 197/200\n",
      "614/614 [==============================] - 0s 81us/step - loss: 0.2355 - acc: 0.9045\n",
      "Epoch 198/200\n",
      "614/614 [==============================] - 0s 78us/step - loss: 0.2365 - acc: 0.9083\n",
      "Epoch 199/200\n",
      "614/614 [==============================] - 0s 75us/step - loss: 0.2378 - acc: 0.9077\n",
      "Epoch 200/200\n",
      "614/614 [==============================] - 0s 78us/step - loss: 0.2340 - acc: 0.9017\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xcfd6445978>"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_final_train, epochs=200, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154/154 [==============================] - 0s 52us/step\n",
      "\n",
      "acc: 90.91%\n"
     ]
    }
   ],
   "source": [
    "y_final_test=pd.get_dummies(data=y_test['y_category'])\n",
    "scores = model.evaluate(X_test, y_final_test)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report\n",
    "#############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### * Basic Explorataory analysis was performed on the dataset.\n",
    "#### * Since there are two dependent variables I have evaluated two models one with heat_loading and the other with cool_loading.\n",
    "#### *  Basic models such as linear regression ,Ridge Regression , lasso regression,decision Tree Regressor were evaluate and their R2 score was computed.\n",
    "#### * As expected Decision Tree has the highest test and train scores and it is over fitting. \n",
    "#### * Ensemble methods such as Random Forest , Bagging , Ada boost were used on both the models and their test and train score were evaluated.\n",
    "#### *  At last using Neural Network I  have classified the combined heating and cooling load (total load) as low,medium,high with a 90.91% Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
